---
title: "MovieLens Data Science Capstone"
author: "Zhang Cheng Hu"
date: "November 2025"
output:
  pdf_document:
    latex_engine: xelatex
header-includes:
  - \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE,
fig.width = 7, fig.height = 4, out.width = "100%", fig.pos="H", out.extra='keepaspectratio')
library(tidyverse)
library(caret)
library(lubridate)
library(data.table)
library(gridExtra)
library(broom)
```

# 1. Introduction / Overview

# 1.1 Background

The MovieLens datasets contain user ratings for movies collected from the MovieLens web site. The 10M dataset contains approximately 10 million ratings along with movie titles and genres.

# 1.2 Goal

Produce a predictive model that estimates a user's rating for a movie and achieves low RMSE on an independent hold-out test set. The report is written for a reader unfamiliar with the dataset; all steps, assumptions, and results are explained.

# 2. Methods & Analysis

This section explains how the data was prepared, major insights from the exploratory analysis, and the modeling approach.

# 2.1 Data Preparation

The raw 10M dataset is provided in two separate files: ratings and movies. We load both components, parse the :: delimiter, convert variables to their appropriate data types, and merge them into a single table.

The project requires that the final hold-out test set contain only user–movie combinations that appear in the training data. To achieve this, we partition the dataset using caret::createDataPartition, reintegrate orphaned rows, and construct the training (edx) and final evaluation (final_holdout_test) sets.

This ensures that the model is never evaluated on users or movies for which it has no information.

```{r load-data}
options(timeout = 120)

dl <- "ml-10M100K.zip"
if (!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
movies_file  <- "ml-10M100K/movies.dat"

if (!file.exists(ratings_file) | !file.exists(movies_file))
  unzip(dl)

ratings <- as.data.frame(
  stringr::str_split(readr::read_lines(ratings_file), fixed("::"), simplify = TRUE),
  stringsAsFactors = FALSE
)

colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
  mutate(
    userId = as.integer(userId),
    movieId = as.integer(movieId),
    rating = as.numeric(rating),
    timestamp = as.integer(timestamp)
  )

movies <- as.data.frame(
  stringr::str_split(readr::read_lines(movies_file), fixed("::"), simplify = TRUE),
  stringsAsFactors = FALSE
)

colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>% mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by="movieId")

set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(movielens$rating, times=1, p=0.1, list=FALSE)

edx <- movielens[-test_index, ]
temp <- movielens[test_index, ]

final_holdout_test <- temp %>%
  semi_join(edx, by="movieId") %>%
  semi_join(edx, by="userId")

removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

edx <- edx %>% mutate(rating_date = as_datetime(timestamp))
```

## 2.2 Exploratory Data Analysis

### Chart 1 — Rating Distribution

```{r rating-dist, fig.cap="Distribution of MovieLens ratings"}
ggplot(edx, aes(rating)) +
  geom_histogram(binwidth=0.5, fill="steelblue", color="white") +
  labs(title="Rating Distribution", x="Rating", y="Count")
```
The ratings are centered around 3–4 stars, with very few extreme ratings. This suggests that predicting the global mean may already yield reasonable accuracy, although personalized effects are likely important.

### Chart 2 — Ratings per Movie (log scale)

```{r ratings-per-movie, fig.cap="Number of ratings per movie (log scale)"}
movie_rating_counts <- edx %>%
  group_by(movieId) %>%
  summarize(count = n(), avg_rating = mean(rating))

ggplot(movie_rating_counts, aes(count)) +
  geom_histogram(bins=50, fill="darkgreen", color="white") +
  scale_x_log10() +
  labs(title="Ratings per Movie (Log Scale)", x="Number of Ratings", y="Frequency")
```
Movies vary drastically in popularity: a few receive tens of thousands of ratings, while many are rated only a handful of times. This imbalance highlights the importance of regularization to prevent overfitting.

### Chart 3 — Ratings per User (log scale)

```{r ratings-per-user, fig.cap="Number of ratings per user (log scale)"}
user_rating_counts <- edx %>%
  group_by(userId) %>%
  summarize(count = n())

ggplot(user_rating_counts, aes(count)) +
  geom_histogram(bins=50, fill="gold", color="white") +
  scale_x_log10() +
  labs(title="Ratings per User (Log Scale)", x="Number of Ratings", y="Frequency")
```
Users also vary widely in rating activity. Some provide only a few ratings; others contribute thousands. Accounting for individual rating tendencies is likely crucial for improving prediction accuracy.

### Chart 4 — Genre Popularity

```{r genre-popularity, fig.cap="Genre frequency in the MovieLens 10M dataset"}
genre_counts <- edx %>%
  separate_rows(genres, sep="\\|") %>%
  count(genres, sort=TRUE)

ggplot(genre_counts, aes(reorder(genres, n), n)) +
  geom_col(fill="tomato") +
  coord_flip() +
  labs(title="Genre Popularity", x="Genre", y="Count")
```
Genres such as Drama and Comedy dominate the dataset, while others are much less common. Because movies often fall into multiple genres, genre effects may be noisy and weak predictors.

Overall, the exploratory analysis suggests that movie-specific behavior, user-specific behavior, and sparse data are central challenges.

# 3. Modeling Approach

We evaluate a sequence of increasingly complex models, each adding a new source of variability.

Model 1: Global Average

Predicts the same rating for all user–movie pairs. This is a baseline used to benchmark improvement.

Model 2: Movie Effects

Accounts for differences in how movies are rated on average. Some movies are consistently rated higher or lower independent of who rates them.

Model 3: Movie + User Effects

Adds personalized tendencies (some users rate generously, others are stricter). This usually produces a substantial reduction in RMSE.

Model 4: Genre Effects

Incorporates average differences between genres. Because movies have multiple genres, we average their genre contributions.

Model 5: Regularized Movie + User Effects

Because movies and users with few ratings can cause extreme bias estimates, we apply shrinkage controlled by a penalty parameter lambda. We tune lambda using the development set to minimize RMSE.

A validation curve confirms that moderate regularization yields the best predictive performance.

### Create development train/test split

```{r train-test-split}
set.seed(1, sample.kind="Rounding")
dev_index <- createDataPartition(edx$rating, p=0.1, list=FALSE)

train_dev <- edx[-dev_index, ]
test_dev  <- edx[dev_index, ]

test_dev <- test_dev %>%
  semi_join(train_dev, by="movieId") %>%
  semi_join(train_dev, by="userId")

removed <- anti_join(edx[dev_index, ], test_dev)
train_dev <- rbind(train_dev, removed)
```
To properly evaluate the different models without contaminating the final results, the edx dataset is divided into two new subsets: a training set (train_dev) and a development test set (test_dev). This allows us to test and compare model performance before evaluating the final model on the untouched final holdout test set.

The split is created using a stratified sampling approach to ensure all rating values remain proportionally represented in both subsets. However, because some movies or users may appear only in one part of the data, the script checks whether all movieId and userId values in test_dev also appear in train_dev. Any rows that fail this condition are returned to the training data.

This careful setup avoids prediction errors (such as trying to estimate ratings for users the model has never seen) and ensures that model tuning and comparison are done in a valid and unbiased way.

### Fit all models

```{r modeling}
RMSE <- function(true, predicted) sqrt(mean((true - predicted)^2))
mu <- mean(train_dev$rating)

# Model 1
pred1 <- rep(mu, nrow(test_dev))
rmse_1 <- RMSE(test_dev$rating, pred1)

# Model 2
movie_effects <- train_dev %>% group_by(movieId) %>% summarize(b_i = mean(rating - mu))
pred2 <- test_dev %>% left_join(movie_effects, by="movieId") %>% mutate(pred = mu + b_i) %>% pull(pred)
rmse_2 <- RMSE(test_dev$rating, pred2)

# Model 3
user_effects <- train_dev %>% left_join(movie_effects, by="movieId") %>% group_by(userId) %>% summarize(b_u = mean(rating - mu - b_i))
pred3 <- test_dev %>% left_join(movie_effects, by="movieId") %>% left_join(user_effects, by="userId") %>% mutate(pred = mu + b_i + b_u) %>% pull(pred)
rmse_3 <- RMSE(test_dev$rating, pred3)

# Model 4 — Genres
genre_avgs <- train_dev %>%
  separate_rows(genres, sep="\\|") %>%
  left_join(movie_effects) %>%
  left_join(user_effects) %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu - b_i - b_u))

test_genres <- test_dev %>%
  separate_rows(genres, sep="\\|") %>%
  left_join(genre_avgs, by="genres") %>%
  group_by(movieId, userId, rating) %>%
  summarize(b_g = mean(b_g, na.rm=TRUE), .groups="drop")

pred4 <- test_dev %>%
  left_join(movie_effects) %>%
  left_join(user_effects) %>%
  left_join(test_genres) %>%
  mutate(b_g = ifelse(is.na(b_g), 0, b_g),
         pred = mu + b_i + b_u + b_g) %>% pull(pred)

rmse_4 <- RMSE(test_dev$rating, pred4)

# Model 5 — Regularized
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(lambda) {
  b_i <- train_dev %>% group_by(movieId) %>% summarize(b_i = sum(rating - mu)/(n()+lambda))
  b_u <- train_dev %>% left_join(b_i, by="movieId") %>% group_by(userId) %>% summarize(b_u = sum(rating - mu - b_i)/(n()+lambda))
  preds <- test_dev %>% left_join(b_i) %>% left_join(b_u) %>% mutate(pred = mu + b_i + b_u) %>% pull(pred)
  RMSE(test_dev$rating, preds)
})

lambda_results <- tibble(lambda=lambdas, rmse=rmses)
best_lambda <- lambda_results$lambda[which.min(lambda_results$rmse)]
rmse_5 <- min(rmses)
```
Once the training and development test sets are prepared, five increasingly sophisticated models are built. Each model aims to reduce prediction error by using more information about the movies, users, and genres.

Global Average Model
This simplest model predicts every rating as the overall mean rating across the entire training dataset. It serves as a baseline for measuring improvement.

Movie Effect Model
This model adds a movie-specific adjustment to the global average. Some movies consistently receive higher or lower ratings, and capturing this effect reduces uncertainty.

Movie + User Effect Model
Users vary in how they rate films—some tend to rate generously, while others are strict. This model captures each user’s rating tendency in addition to movie effects.

Movie + User + Genre Model
Movies can belong to multiple genres. The model estimates the average rating contribution of each genre and combines it with movie and user effects. The impact is typically smaller because genre categories overlap.

Regularized Movie + User Model
Regularization addresses a critical issue: many movies and users have very few ratings. Without regularization, the model may overfit to these small samples.
This model applies a penalty term to shrink effects toward zero when data is sparse, producing more reliable predictions.

Each model generates a set of predictions for the test_dev set, and their performance is compared using RMSE (Root Mean Square Error). As model complexity increases, the goal is to observe a consistent reduction in RMSE.

### Lambda tuning curve

```{r lambda-plot, fig.cap="Lambda tuning curve"}
lambda_results %>%
  ggplot(aes(lambda, rmse)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = best_lambda, linetype="dashed", color="red") +
  labs(title="Lambda Tuning Curve", x="Lambda", y="RMSE")
```
The strength of the regularization penalty is controlled by a parameter called lambda.

If lambda is too small the model overfits (too much trust in small sample sizes).

If lambda is too large the model underfits (effects are overly shrunk).

To find the optimal lambda, the script evaluates a sequence of candidate lambda values ranging from 0 to 10. For each lambda, the model is rebuilt and its RMSE is calculated using the development test set.

The results are summarized in a tuning curve, which plots lambda on the x-axis and RMSE on the y-axis.
The best lambda is the one that achieves the lowest RMSE, striking the right balance between bias and variance.

This optimal lambda is then used when retraining the final model on the full edx dataset before evaluating performance on the final holdout test set.

# 4. Results

### RMSE Comparison Table

```{r rmse-table}
model_results <- tibble(
  Model = c(
    "Global Average", "Movie Effect", "Movie + User Effect",
    "Movie + User + Genre Effect", "Regularized Movie + User"
  ),
  RMSE = c(rmse_1, rmse_2, rmse_3, rmse_4, rmse_5)
)

knitr::kable(model_results, digits=5, caption="RMSE Comparison of All Models")
```

After fitting all models on the development data, we compare their RMSE values.

The results show clear improvement with model complexity:

- The global average performs the worst.
- The movie effect reduces RMSE substantially.
- Adding user effects further improves accuracy, indicating that individual user behavior is highly important.
- Genre effects offer only a small marginal improvement.

The regularized movie + user model performs best, confirming that shrinkage reduces overfitting.

The RMSE table summarizes the progression.

The optimal lambda identified during tuning is incorporated into the final model.

Finally, we evaluate the selected model on the final holdout test set. This RMSE is included in the report.

# 5. Final Hold-Out Test Evaluation

In this last step, the best model is trained again using all of edx so it has the most complete information.
It then predicts the ratings in the final holdout test set, which the model has never seen before.

The model uses:

- the overall average rating
- movie effects
- user effects

to make each prediction.

```{r final-model}
mu_full <- mean(edx$rating)

b_i_full <- edx %>% group_by(movieId) %>% summarize(b_i=sum(rating-mu_full)/(n()+best_lambda))
b_u_full <- edx %>% left_join(b_i_full) %>% group_by(userId) %>% summarize(b_u=sum(rating-mu_full-b_i)/(n()+best_lambda))

final_predictions <- final_holdout_test %>%
  left_join(b_i_full) %>%
  left_join(b_u_full) %>%
  mutate(
    b_i = ifelse(is.na(b_i), 0, b_i),
    b_u = ifelse(is.na(b_u), 0, b_u),
    pred = mu_full + b_i + b_u
  ) %>% pull(pred)

final_rmse <- RMSE(final_holdout_test$rating, final_predictions)

final_rmse
```

The final RMSE shows how well the model performs on truly unseen data.

# 6. Conclusion

This project developed a predictive model for MovieLens ratings using a structured sequence of statistical techniques. We demonstrated that both movie-specific and user-specific factors strongly influence ratings, while genre provides limited additional explanatory power. Regularization was essential to prevent overfitting caused by sparse user–movie combinations.

The final regularized model achieved strong out-of-sample RMSE on the official hold-out set, indicating good generalization performance.

# 7. References

- GroupLens Research. (n.d.). MovieLens Dataset. https://grouplens.org/datasets/movielens/
- HarvardX PH125.9x Course Materials.
- Regularization concepts referenced from James et al. An Introduction to Statistical Learning.