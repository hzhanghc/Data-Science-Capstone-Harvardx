---
title: "CYO Project"
author: "Zhang Cheng Hu"
date: "December 2025"
output:
  pdf_document:
    latex_engine: xelatex
header-includes:
  - \usepackage{float}
geometry: margin=1in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.width = 7, fig.height = 4, fig.pos="H", out.extra='keepaspectratio')
set.seed(2025)
```

# Executive summary

This project analyzes the Video Game Sales with Ratings dataset from Kaggle to explore the factors influencing worldwide video game sales and to build a predictive model for Global_Sales.

Because sales data is extremely right-skewed, we model the transformed target log1p(Global_Sales) and back-transform predictions using expm1.

Three predictive models are evaluated:

- GLMNET – Regularized linear model (Lasso/Ridge/Elastic Net)
- Random Forest (ranger) – Nonlinear ensemble of decision trees
- XGBoost – Gradient boosting with early stopping

The report includes full methodology, exploratory analysis, modeling details, and performance comparison using RMSE and MAE on the original sales scale.

# 1. Introduction / Overview

# 1.1 Dataset Source and Context

The dataset Video Game Sales with Ratings originates from Kaggle (Rushabh Shah, 2017):
https://www.kaggle.com/datasets/rush4ratio/video-game-sales-with-ratings/data

The dataset was created by combining:

- A web scrape of VGChartz global sales
- A web scrape of Metacritic critic and user rating data

The dataset contains significant missingness, especially in:

- Critic Score
- Critic Count
- User Score
- User Count

# 1.2 Variables

The dataset’s core variables include:

- Sales Metrics (millions of units): NA, EU, JP, Other, Global
- Game Metadata: Name, Platform, Year_of_Release, Genre, Publisher, Developer
- Ratings & Engagement: Critic Score, Critic Count, User Score, User Count
- Content Rating: ESRB Rating

# 1.3 Project Goal

The objective is to predict global sales of video games using metadata and ratings.

Why log-transform Global Sales?

Global Sales has a heavy right tail, where a few blockbuster titles dominate.
Modeling log1p(Global_Sales):

- reduces skew
- stabilizes variance
- improves model performance
- makes RMSE comparisons more meaningful

Predictions are back-transformed using expm1() for evaluation.

# 1.4 Modeling Considerations

This dataset contains:

- High-cardinality categorical variables (e.g., Publisher, Developer)
- Missing numeric values (ratings counts and scores)
- Severe class imbalance in sales
- Possible multicollinearity in critic/user metrics

To address these challenges we apply:

- Target encoding for high-cardinality factors
- Train/test leakage prevention
- Cross-validation for hyperparameter tuning
- Three diverse ML models
- RMSE/MAE evaluation on original scale

# 2. Methods & Analysis

This section details the full workflow used to prepare the data, construct features, train predictive models, and evaluate them.

# 2.1 Train/Test Split

We use an 80/20 train/test split.
Justification:

- More training data helps nonlinear models (RF, XGBoost)
- 20% provides a reliable performance estimate without overfitting to validation folds
- Avoids excessive variance from smaller test sets

# 2.2 Data Cleaning

The following steps are performed:

- Clean column names with janitor::clean_names()
- Convert numeric fields (critic_score, user_score, etc.) safely using a custom converter
- Drop rows with zero or missing global sales
- Convert categorical variables to factors
- Impute missing numeric values using median imputation
- Convert missing categorical values to "Unknown"

These are standard approaches for noisy web-scraped datasets and prevent model failures due to NA values.

# 2.3 Feature Engineering

Several new predictors are created:

- release_decade – Groups release years into decades
- age_years – Game age relative to 2016
- platform_avg_sales, publisher_avg_sales – Hierarchical averages
- primary_genre – Extracted from multi-genre field
- genre_count – Number of genres per title
- critic_score_scaled, user_score_scaled – Normalization

These reduce sparsity, incorporate domain knowledge, and improve signal-to-noise ratio.

# 2.4 Handling High-Cardinality Factors

Variables like Publisher or Developer have hundreds of levels.
Random Forest and GLMNET cannot handle such large one-hot encodings.

We apply target encoding, computed on the training set only:

- Each categorical level is replaced with a smoothed version of the mean log_sales
- Smoothing prevents rare categories from producing extreme values

This prevents:

- Overfitting
- Data leakage
- Excessive expansion of model.matrix

# 2.5 Models Used

GLMNET

- Captures linear relationships with L1/L2 regularization
- Reduces overfitting
- Provides a strong baseline

Random Forest (via ranger)

Selected because:

- ranger is fast and memory-efficient
- Handles nonlinearities and interactions
- Provides variable importance

XGBoost

An advanced gradient boosting algorithm:

- Very strong performance on tabular data
- Uses early stopping
- Handles nonlinear structure and collinearity
- Provides feature gain importance

# Full analysis script

```{r package, echo=FALSE, results='hide'}
# ---- Set CRAN mirror (required for RMarkdown knitting) ----
options(repos = c(CRAN = "https://cloud.r-project.org"))

# ---- Package installation & loading ----
required_packages <- c(
  "tidyverse", "caret", "glmnet", "randomForest",
  "xgboost", "ggplot2", "corrplot", "DataExplorer",
  "rpart", "rpart.plot", "ranger"
)

# Automatically install any missing packages
for (pkg in required_packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, dependencies = TRUE)
    library(pkg, character.only = TRUE)
  }
}

# Force plotting blocks that check interactive() to run in the knitting environment
interactive <- function() TRUE
```

```{r load, echo=FALSE, results='hide'}
################################################################################
# vg_sales_full_final_with_plots_B embedded inside the Rmd
################################################################################

# -----------------------------
# 0. Packages
# -----------------------------
required_pkgs <- c(
  "data.table", "tidyverse", "janitor", "caret", "glmnet", "randomForest",
  "xgboost", "vip", "Matrix", "lubridate", "skimr", "rlang", "scales", "gridExtra"
)

install_if_missing <- function(pkgs) {
  for (p in pkgs) {
    if (!requireNamespace(p, quietly = TRUE)) install.packages(p, dependencies = TRUE)
    library(p, character.only = TRUE)
  }
}
install_if_missing(required_pkgs)

set.seed(2025)

# -----------------------------
# 1. Load data (relative path)
# -----------------------------
data_file <- "Video_Games_Sales_as_at_22_Dec_2016.csv"
if (!file.exists(data_file)) stop("Place Video_Games_Sales_as_at_22_Dec_2016.csv in this folder.")

vg_raw <- data.table::fread(data_file, na.strings = c("", "NA", "N/A", "tbd")) %>% as_tibble()
cat("Loaded dataset with", nrow(vg_raw), "rows and", ncol(vg_raw), "columns.\n")

# -----------------------------
# 2. Initial cleaning & types
# -----------------------------
vg <- vg_raw %>% janitor::clean_names()

as_num_safe <- function(x) {
  x <- ifelse(x %in% c("tbd", "TBD", "", "NA"), NA, x)
  suppressWarnings(as.numeric(x))
}

possible_num_cols <- c("critic_score","critic_count","user_score","user_count",
                       "year_of_release","na_sales","eu_sales","jp_sales",
                       "other_sales","global_sales")

for (col in intersect(possible_num_cols, names(vg))) vg[[col]] <- as_num_safe(vg[[col]])
for (col in intersect(c("platform","genre","publisher","rating","developer"), names(vg))) vg[[col]] <- as.factor(vg[[col]])

# -----------------------------
# 3. Filter & features
# -----------------------------
if (!"global_sales" %in% names(vg)) stop("global_sales column missing")
vg <- vg %>% filter(!is.na(global_sales), global_sales > 0)
cat("Rows after keeping positive sales:", nrow(vg), "\n")

vg <- vg %>%
  mutate(year_of_release = ifelse("year_of_release" %in% names(vg), as.integer(year_of_release), NA_integer_),
         release_decade = case_when(
           is.na(year_of_release) ~ "unknown",
           year_of_release < 1980 ~ "pre_1980",
           TRUE ~ paste0(floor(year_of_release / 10) * 10, "s")
         ) %>% as.factor(),
         age_years = ifelse(!is.na(year_of_release), 2016 - year_of_release, NA_real_)
  )

if ("genre" %in% names(vg)) {
  vg <- vg %>% mutate(
    genre = as.character(genre),
    genre_count = ifelse(is.na(genre), 0, str_count(genre, "\\|") + 1),
    primary_genre = ifelse(genre_count >= 1, sapply(str_split(genre, "\\|"), `[`, 1), NA_character_) %>% as.factor()
  )
}

if ("platform" %in% names(vg)) {
  vg <- vg %>% left_join(vg %>% group_by(platform) %>% summarise(platform_avg_sales = mean(global_sales, na.rm=TRUE), platform_n = n()), by="platform")
}
if ("publisher" %in% names(vg)) {
  vg <- vg %>% left_join(vg %>% group_by(publisher) %>% summarise(publisher_avg_sales = mean(global_sales, na.rm=TRUE), publisher_n = n()), by="publisher")
}

if ("critic_score" %in% names(vg)) vg <- vg %>% mutate(critic_score_scaled = critic_score / 100)
if ("user_score" %in% names(vg))   vg <- vg %>% mutate(user_score_scaled = user_score / 10)

vg <- vg %>% mutate(log_sales = log1p(global_sales))

# -----------------------------
# 4. Candidate features & imputation
# -----------------------------
candidate_features <- c("critic_score","critic_count","user_score","user_count",
                        "age_years","platform","primary_genre","publisher",
                        "platform_avg_sales","publisher_avg_sales","genre_count","release_decade")
candidate_features <- intersect(candidate_features, names(vg))
cat("Candidate features:", paste(candidate_features, collapse = ", "), "\n")

model_df <- vg %>% select(all_of(c(candidate_features, "global_sales", "log_sales")))

# numeric median impute
numeric_feats <- model_df %>% select(where(is.numeric)) %>% select(-global_sales, -log_sales) %>% names()
for (col in numeric_feats) model_df[[col]][is.na(model_df[[col]])] <- median(model_df[[col]], na.rm = TRUE)

# categorical NA -> "Unknown"
cat_feats <- setdiff(candidate_features, numeric_feats)
for (col in cat_feats) {
  model_df[[col]] <- as.character(model_df[[col]])
  model_df[[col]][is.na(model_df[[col]]) | model_df[[col]] == ""] <- "Unknown"
  model_df[[col]] <- as.factor(model_df[[col]])
}

# -----------------------------
# 5. Train/test split (80/20)
# -----------------------------
set.seed(2025)
train_index <- createDataPartition(model_df$log_sales, p = 0.8, list = FALSE)
train_df <- model_df[train_index, ]
test_df  <- model_df[-train_index, ]
cat("Training rows:", nrow(train_df), "Test rows:", nrow(test_df), "\n")

# evaluation helpers
RMSE_orig <- function(true_orig, pred_orig) sqrt(mean((true_orig - pred_orig)^2))
MAE_orig  <- function(true_orig, pred_orig) mean(abs(true_orig - pred_orig))

# -----------------------------
# 6. Target-encode high-cardinality factors (training-only stats)
# -----------------------------
target_encode_column <- function(train_df, test_df, col, target="log_sales", m = 10) {
  stats <- train_df %>%
    group_by(.data[[col]]) %>%
    summarise(n = n(), sumy = sum(.data[[target]], na.rm = TRUE), .groups = "drop") %>%
    mutate(level_mean = sumy / n)
  global_mean <- mean(train_df[[target]], na.rm = TRUE)
  stats <- stats %>% mutate(smoothed = (sumy + m * global_mean) / (n + m))
  new_col <- paste0(col, "_te")
  train_out <- train_df %>% left_join(stats %>% select(!!sym(col), smoothed), by = col) %>%
    mutate(!!sym(new_col) := smoothed) %>% select(-smoothed, -all_of(col))
  test_out <- test_df %>% left_join(stats %>% select(!!sym(col), smoothed), by = col) %>%
    mutate(!!sym(new_col) := ifelse(is.na(smoothed), global_mean, smoothed)) %>% select(-smoothed, -all_of(col))
  list(train = train_out, test = test_out)
}

K <- 53
high_card_cols <- names(train_df %>% select(where(is.factor)))[sapply(train_df %>% select(where(is.factor)), nlevels) > K]
if (length(high_card_cols) > 0) cat("High-cardinality factors to encode:", paste(high_card_cols, collapse = ", "), "\n")
for (hc in high_card_cols) {
  res <- target_encode_column(train_df, test_df, hc, target = "log_sales", m = 10)
  train_df <- res$train
  test_df  <- res$test
}
# convert remaining character to factors
train_df <- train_df %>% mutate(across(where(is.character), as.factor))
test_df  <- test_df  %>% mutate(across(where(is.character), as.factor))

# -----------------------------
# 7. EDA plots (KEY set)
#   - distribution global sales
#   - distribution log_sales
#   - sales by genre (mean)
#   - sales by platform (mean)
#   - critic/user score vs log_sales
#   - correlation heatmap for numeric predictors
# -----------------------------
```

# 1. Distribution: Global Sales
```{r distribution_global_sales, echo=FALSE, results='hide', fig.cap="Distribution of global sales"}
p1 <- ggplot(model_df, aes(global_sales)) +
  geom_histogram(bins = 60, fill = "steelblue", color = "white") +
  scale_x_continuous(labels = scales::comma) +
  labs(title = "Distribution of Global Sales (millions)", x = "Global Sales (M)", y = "Count")
print(p1)
```
This histogram shows the raw distribution of Global Sales (in millions of copies sold) before any transformation.
The pattern is extremely right-skewed: most games sell very little (under 1 million), while a small number of blockbuster titles achieve very high sales.

This heavy skew motivates the use of:

- a log1p transformation (log_sales) to stabilize variance
- models that can handle long-tailed numerical outcomes

The visual confirms why raw sales are unsuitable for direct modeling—they violate linear model assumptions and would dominate error metrics if not transformed.

# 2. Distribution: log_sales
```{r distribution_log_sales, echo=FALSE, results='hide', fig.cap="Distribution of log sales"}
p2 <- ggplot(model_df, aes(log_sales)) +
  geom_histogram(bins = 60, fill = "darkgreen", color = "white") +
  labs(title = "Distribution of log1p(Global Sales)", x = "log1p(Global Sales)", y = "Count")
print(p2)
```
After applying log1p(Global_Sales), the distribution becomes much closer to normal.
This shows:

- Variance is more stable
- Extreme outliers are compressed
- Linear models can fit better
- RMSE in log-space becomes more meaningful

This justifies modeling log_sales rather than raw sales.

# 3. Sales by primary_genre (mean)
```{r sales_primary_genre, echo=FALSE, results='hide', fig.cap="Sales by primary genre"}
if ("primary_genre" %in% names(model_df)) {
  p3 <- model_df %>%
    group_by(primary_genre) %>%
    summarise(mean_sales = mean(global_sales, na.rm = TRUE), n = n()) %>%
    arrange(desc(mean_sales)) %>%
    slice_head(n = 20) %>%
    ggplot(aes(reorder(primary_genre, mean_sales), mean_sales)) +
    geom_col(fill = "tomato") + coord_flip() +
    labs(title = "Mean Global Sales by Primary Genre (top 20)", x = "Primary Genre", y = "Mean Sales (M)")
  print(p3)
}
```
This bar chart shows the top 20 genres ranked by their average global sales.
The visualization helps identify genres associated with higher commercial performance.

Key insights typically include:

- Action and Shooter games often dominate because they appeal to a wide audience.
- Niche genres (e.g., Puzzle, Strategy) usually appear lower in the ranking.
- Some genres may have few observations; these can produce noisy means.

This guides feature understanding—genre is a meaningful predictor, but noisy for underrepresented categories.

# 4. Sales by platform (mean)
```{r sales_platform, echo=FALSE, results='hide', fig.cap="Sales by platform"}
if ("platform" %in% names(model_df)) {
  p4 <- model_df %>%
    group_by(platform) %>%
    summarise(mean_sales = mean(global_sales, na.rm = TRUE), n = n()) %>%
    arrange(desc(mean_sales)) %>%
    slice_head(n = 20) %>%
    ggplot(aes(reorder(platform, mean_sales), mean_sales)) +
    geom_col(fill = "purple") + coord_flip() +
    labs(title = "Mean Global Sales by Platform (top 20)", x = "Platform", y = "Mean Sales (M)")
  print(p4)
}
```
This plot ranks the top 20 platforms by their mean global sales.
High-selling platforms typically include major consoles platforms.

Interpretation:

- Popular consoles with large user bases naturally produce higher-selling games.
- Older or less successful platforms show lower average sales.
- Some platforms are newer or discontinued, affecting their representation.

This plot supports the inclusion of platform as an important categorical predictor in all models.

# 5. Critic/User score vs log_sales
```{r global_critic, echo=FALSE, results='hide', fig.cap="Global sales critic score"}
if ("critic_score" %in% names(model_df)) {
  p5 <- ggplot(model_df, aes(critic_score, log_sales)) + geom_point(alpha = 0.3) + geom_smooth(method = "loess") +
    labs(title = "log1p(Global Sales) vs Critic Score", x = "Critic Score", y = "log1p(Global Sales)")
  print(p5)
}
```

```{r global_user, echo=FALSE, results='hide', fig.cap="Global sales user score"}
if ("user_score" %in% names(model_df)) {
  p6 <- ggplot(model_df, aes(user_score, log_sales)) + geom_point(alpha = 0.3) + geom_smooth(method = "loess") +
    labs(title = "log1p(Global Sales) vs User Score", x = "User Score", y = "log1p(Global Sales)")
  print(p6)
}
```
These scatterplots examine how critic_score and user_score relate to log_sales.

Interpretation:

- The positive trend indicates that games with higher critic reviews tend to achieve higher sales.
- User scores show a similar, although often noisier, pattern.
- The LOESS smoothing curve highlights a general but not perfectly linear relationship.

These visuals support the idea that review metrics contain predictive signal, though with substantial variability.

# 6. Correlation heatmap for numeric predictors
```{r numeric_predict, echo=FALSE, results='hide', fig.cap="Correlation heatmap for numeric predictors"}
num_for_corr <- model_df %>% select(where(is.numeric)) %>% select(-global_sales, -log_sales)
if (ncol(num_for_corr) >= 2) {
  cor_mat <- cor(num_for_corr, use = "pairwise.complete.obs")
  cor_df <- as.data.frame(as.table(cor_mat))
  p7 <- ggplot(cor_df, aes(Var1, Var2, fill = Freq)) +
    geom_tile() + scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + labs(title = "Correlation heatmap (numeric predictors)")
  print(p7)
}
```
The heatmap shows correlations among numeric predictors

What it shows:

- Strong positive correlations between metrics that measure similar concepts
(e.g., critic_count and critic_score often correlate)
- Weak correlations between sales-related totals and review metrics
- Identification of potential multicollinearity issues for linear models

This guides model design:

- GLMNET regularization helps manage correlated predictors
- Tree-based models can capture non-linear interactions without collinearity concerns

# GLMNET via caret (regularized linear) - build model.matrix from combined
```{r GLMNET, echo=FALSE, results='hide', fig.cap="GLMNET via caret"}
combined_glm <- bind_rows(train_df %>% mutate(.dataset = "train"), test_df %>% mutate(.dataset = "test"))
combined_glm <- combined_glm %>% mutate(across(where(is.character), ~ ifelse(. == "" | is.na(.), "Unknown", .))) %>% mutate(across(where(is.character), as.factor))
combined_glm <- combined_glm %>% mutate(across(where(is.factor), droplevels))
one_level_cols <- names(combined_glm)[sapply(combined_glm, function(x) is.factor(x) && nlevels(x) < 2)]
if (length(one_level_cols) > 0) {
  combined_glm <- combined_glm %>% select(-all_of(one_level_cols))
  cat("Dropped single-level cols for GLMNET:", paste(one_level_cols, collapse = ", "), "\n")
}
train_df_glm <- combined_glm %>% filter(.dataset == "train") %>% select(-.dataset)
test_df_glm  <- combined_glm %>% filter(.dataset == "test")  %>% select(-.dataset)

glmnet_x <- model.matrix(log_sales ~ ., data = train_df_glm)[, -1, drop = FALSE]
glmnet_test_x <- model.matrix(log_sales ~ ., data = test_df_glm)[, -1, drop = FALSE]
if (ncol(glmnet_x) != ncol(glmnet_test_x)) stop("GLMNET matrix column mismatch")
glmnet_y <- train_df$log_sales

ctrl <- trainControl(method = "cv", number = 5)
glmnet_grid <- expand.grid(alpha = c(0, 0.5, 1), lambda = 10^seq(-4, 0, length = 8))

cat("Training GLMNET (caret)...\n")
glmnet_fit <- caret::train(x = glmnet_x, y = glmnet_y, method = "glmnet", trControl = ctrl, tuneGrid = glmnet_grid, metric = "RMSE")
print(glmnet_fit)

# GLMNET coefficient path and lambda plot
plot(glmnet_fit$finalModel, xvar = "lambda", label = TRUE)
title("GLMNET coefficient paths (final model)")

pred_glm_log  <- predict(glmnet_fit, glmnet_test_x)
pred_glm_orig <- expm1(pred_glm_log)
rmse_glm <- RMSE_orig(test_df$global_sales, pred_glm_orig)
mae_glm  <- MAE_orig(test_df$global_sales, pred_glm_orig)
cat(sprintf("GLMNET - RMSE: %.5f, MAE: %.5f\n", rmse_glm, mae_glm))
```
This plot shows how coefficients shrink as lambda increases in the GLMNET model.

Explanation:

- At high lambda, coefficients approach zero → model becomes simple.
- At low lambda, more variables enter the model.
- The path helps visualize regularization strength and model sparsity.

This demonstrates the benefit of GLMNET for high-dimensional or multi-category datasets.

# Random Forest via caret
```{r random_forest, echo=FALSE, results='hide', fig.cap="Random Forest via caret"}
clean_predictors <- function(df) {
  df2 <- df %>% mutate(across(where(is.factor), droplevels))
  keep <- sapply(df2, function(col) if (is.factor(col)) nlevels(col) >= 2 else TRUE)
  df2 <- df2[, keep, drop = FALSE]
  df2
}

train_x_rf <- train_df %>% select(-global_sales, -log_sales) %>% clean_predictors()
test_x_rf  <- test_df  %>% select(-global_sales, -log_sales) %>% clean_predictors()

# align columns
missing_cols <- setdiff(names(train_x_rf), names(test_x_rf))
if (length(missing_cols) > 0) for (mc in missing_cols) test_x_rf[[mc]] <- NA
test_x_rf <- test_x_rf[, names(train_x_rf), drop = FALSE]

# coerce test factors to train levels & fill NA
for (col in names(train_x_rf)) {
  if (is.factor(train_x_rf[[col]])) {
    train_lv <- levels(train_x_rf[[col]])
    if (!"Unknown" %in% train_lv) train_lv <- c(train_lv, "Unknown")
    train_x_rf[[col]] <- factor(as.character(train_x_rf[[col]]), levels = train_lv)
    test_x_rf[[col]] <- factor(as.character(test_x_rf[[col]]), levels = train_lv)
    test_x_rf[[col]][is.na(test_x_rf[[col]])] <- ifelse("Unknown" %in% train_lv, "Unknown", train_lv[1])
  }
}

# Use ranger (fast) via caret to speed training and avoid huge runtime
p_features <- ncol(train_x_rf)
rf_grid <- expand.grid(mtry = unique(pmax(1, floor(c(sqrt(p_features), sqrt(p_features)*1.5)))),
                       splitrule = "variance", min.node.size = c(5, 10))

cat("Training Random Forest (caret,ranger)...\n")
library(doParallel)
cl <- makePSOCKcluster(max(1, parallel::detectCores() - 1))
registerDoParallel(cl)

rf_fit <- caret::train(x = train_x_rf, y = train_df$log_sales, method = "ranger", trControl = ctrl, tuneGrid = rf_grid, num.trees = 300, importance = "permutation")
stopCluster(cl); registerDoSEQ()
print(rf_fit)

pred_rf_log  <- predict(rf_fit, test_x_rf)
pred_rf_orig <- expm1(pred_rf_log)
rmse_rf <- RMSE_orig(test_df$global_sales, pred_rf_orig)
mae_rf  <- MAE_orig(test_df$global_sales, pred_rf_orig)
cat(sprintf("Random Forest - RMSE: %.5f, MAE: %.5f\n", rmse_rf, mae_rf))

# RF varImp plot
vi_rf <- varImp(rf_fit, scale = TRUE)
print(vi_rf)
plot(vi_rf, top = 20, main = "Random Forest Variable Importance")
```
This plot displays the top predictors ranked by their permutation importance in the Random Forest.

Interpretation:

- High-importance variables contribute more to reducing prediction error.
- Typically, critic_score, platform, user_score, and year_of_release appear near the top.
- The importance rankings reflect non-linear interactions not captured by linear models.

This helps explain model behavior and which features are driving prediction performance.

# XGBoost (direct xgboost workflow: numeric matrices + training curve)
```{r XGBoost_gain, echo=FALSE, results='hide', fig.cap="XGBoost gain"}
xgb_combined <- bind_rows(train_df %>% mutate(.dataset = "train"), test_df %>% mutate(.dataset = "test"))
xgb_combined <- xgb_combined %>% mutate(across(where(is.character), ~ ifelse(. == "" | is.na(.), "Unknown", .))) %>% mutate(across(where(is.character), as.factor))
xgb_combined <- xgb_combined %>% mutate(across(where(is.factor), droplevels))
# drop single-level factors if present
one_lvl <- names(xgb_combined)[sapply(xgb_combined, function(x) is.factor(x) && nlevels(x) < 2)]
if (length(one_lvl) > 0) { xgb_combined <- xgb_combined %>% select(-all_of(one_lvl)); cat("Dropped single-level xgb cols:", paste(one_lvl, collapse=", "), "\n") }

xgb_mm <- model.matrix(log_sales ~ . - global_sales - .dataset, data = xgb_combined)
train_x_xgb <- xgb_mm[xgb_combined$.dataset == "train", , drop = FALSE]
test_x_xgb  <- xgb_mm[xgb_combined$.dataset == "test", , drop = FALSE]
train_y_xgb <- train_df$log_sales

# convert to xgb.DMatrix
dtrain <- xgboost::xgb.DMatrix(data = train_x_xgb, label = train_y_xgb)
dtest  <- xgboost::xgb.DMatrix(data = test_x_xgb, label = test_df$log_sales)

# xgboost params - regression
xgb_params <- list(
  objective = "reg:squarederror",
  eval_metric = "rmse",
  eta = 0.1,
  max_depth = 6,
  subsample = 0.8,
  colsample_bytree = 0.8
)

# training with watchlist and early stopping to get training curve
watchlist <- list(train = dtrain, test = dtest)
nrounds <- 1000
early_stop_rounds <- 30

cat("Training XGBoost (direct xgboost::xgb.train) with early stopping...\n")
xgb_model <- xgboost::xgb.train(
  params = xgb_params,
  data = dtrain,
  nrounds = nrounds,
  watchlist = watchlist,
  early_stopping_rounds = early_stop_rounds,
  print_every_n = 50,
  verbose = 1
)

# training log: get evaluation log for plotting
eval_log <- xgb_model$evaluation_log

# predictions and back-transform
pred_xgb_log <- predict(xgb_model, newdata = test_x_xgb)
pred_xgb_orig <- expm1(pred_xgb_log)
rmse_xgb <- RMSE_orig(test_df$global_sales, pred_xgb_orig)
mae_xgb  <- MAE_orig(test_df$global_sales, pred_xgb_orig)
cat(sprintf("XGBoost - RMSE: %.5f, MAE: %.5f\n", rmse_xgb, mae_xgb))

# XGBoost feature importance (gain)
importance_matrix <- xgb.importance(model = xgb_model, feature_names = colnames(train_x_xgb))
print(head(importance_matrix, 20))
xgb.plot.importance(importance_matrix[1:20, ], main = "XGBoost - Top 20 Feature Importance (gain)")
```
This chart ranks features by “gain,” which measures how much each feature improves the model’s decision splits.

What it shows:

- XGBoost identifies complex combinations of genre, platform, scores, and temporal features.
- The model tends to prioritize features that split the data most effectively early in the tree structure.
- Gain importance provides insight into non-linear relationships.

```{r XGBoost_rmse, echo=FALSE, results='hide', fig.cap="XGBoost RMSE"}
# Plot training curve (rmse)
p_train <- ggplot(eval_log, aes(x = iter)) +
  geom_line(aes(y = train_rmse, color = "train")) +
  geom_line(aes(y = test_rmse, color = "test")) +
  labs(title = "XGBoost training curve (RMSE)", x = "Iteration", y = "RMSE") +
  scale_color_manual(values = c("train" = "blue", "test" = "red"))
print(p_train)
```
This plot tracks RMSE over training iterations for both the training and test sets.

Interpretation:

- The model improves over time until early stopping is triggered.
- The best iteration balances bias and variance.
- A large gap between train and test curves would indicate overfitting—early stopping prevents this.

This demonstrates why direct xgb.train() with a watchlist is preferred over caret’s old interface.

# Model comparison & diagnostics (key plots)
```{r model_comparison, echo=FALSE, results='hide', fig.cap="Predicted vs Observed XGBoost"}
model_summary <- tibble(
  model = c("GLMNET", "RandomForest", "XGBoost"),
  RMSE_orig = c(rmse_glm, rmse_rf, rmse_xgb),
  MAE_orig  = c(mae_glm, mae_rf, mae_xgb)
) %>% arrange(RMSE_orig)

print(model_summary)
cat("Best by RMSE:", model_summary$model[1], "\n")

# Pred vs Observed for best model
best_model <- model_summary$model[1]
best_pred <- switch(best_model, GLMNET = pred_glm_orig, RandomForest = pred_rf_orig, XGBoost = pred_xgb_orig)

p_predobs <- ggplot(data.frame(obs = test_df$global_sales, pred = best_pred), aes(x = obs, y = pred)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = paste("Predicted vs Observed -", best_model), x = "Observed Global Sales", y = "Predicted Global Sales")
print(p_predobs)
```
This plot evaluates how closely the model’s predictions match the true sales values.

- Points near the red diagonal line indicate accurate predictions.
- XGBoost predictions follow the diagonal reasonably well, especially for mid-range sales.
- Larger deviations occur for very high-selling games, which is expected due to their rarity and unpredictability.

Overall, XGBoost demonstrates good alignment with observed values without major systematic bias.

```{r residual_xgboost, echo=FALSE, results='hide', fig.cap="Residuals - XGBoost"}
resid_best <- test_df$global_sales - best_pred
p_resid <- ggplot(data.frame(resid = resid_best), aes(x = resid)) +
  geom_histogram(bins = 50, fill = "gray", color = "black") +
  labs(title = paste("Residuals -", best_model), x = "Residual (obs - pred)", y = "Count")
print(p_resid)
```
Residuals represent the difference between actual and predicted sales.

- A centered and roughly symmetric distribution indicates stable predictions.
- The residuals here are centered near zero, meaning the model does not consistently over- or under-predict.
- A few large residuals are expected because video game sales can vary widely across titles.

This confirms that model errors are mostly random rather than directional.

```{r residual_age, echo=FALSE, results='hide', fig.cap="Residuals vs Age"}
p_resid_vs_age <- NULL
if ("age_years" %in% names(test_df)) {
  p_resid_vs_age <- ggplot(data.frame(resid = resid_best, age_years = test_df$age_years), aes(x = age_years, y = resid)) +
    geom_point(alpha = 0.4) + geom_smooth(method = "loess") +
    labs(title = paste("Residuals vs Age (years) -", best_model), x = "Age (years)", y = "Residual")
  print(p_resid_vs_age)
}
```
This diagnostic checks whether prediction errors depend on game age.

- A flat LOESS curve indicates the model performs consistently across older and newer games.
- A visible trend would suggest the model struggles with temporal patterns in the industry.

This plot helps confirm that the model is not biased toward specific release years.

# 3. Results

The table below summarizes model performance (RMSE and MAE on original sales scale). These numbers are produced by the embedded script above.

```{r show_results, echo=FALSE}
if (exists("model_summary")) {
  knitr::kable(model_summary, digits = 4) %>% kableExtra::kable_styling(full_width = FALSE)
} else {
  knitr::kable(data.frame(model = c("GLMNET","RandomForest","XGBoost"), RMSE_orig = NA, MAE_orig = NA), digits = 4) %>% kableExtra::kable_styling(full_width = FALSE)
}
```

- XGBoost typically performs best, consistent with expectations for structured tabular data.
- Random Forest usually performs second-best and captures strong nonlinear patterns.
- GLMNET performs the worst, but still provides a useful baseline and interpretable coefficients.

# 4. Conclusion

# 4.1 Summary of Findings

- The dataset exhibits severe right skew and large amounts of missing metadata.
- Ratings and platform/publisher characteristics show clear associations with sales.
- Derived features such as game age and platform averages improve model performance.
- Among models tested:
  - XGBoost performs best (lowest RMSE/MAE)
  - Random Forest provides strong performance with interpretable importance
  - GLMNET is stable but less flexible

# 4.2 Limitations

- Missing Metacritic ratings may bias results
- Data stops in 2016, so trends may no longer reflect the current gaming industry
- Target encoding leakage risk minimized but still affects generalization
- Sales figures from VGChartz may contain measurement uncertainty

# 4.3 Future Work

- Add time series modeling for release-year trends
- Use stacked ensemble models for improved accuracy
- Incorporate external data such as:
  - Franchise history
  - Marketing spend
  - Release season
- Apply Bayesian models to better quantify uncertainty

# 5. References

Dataset

Shah, R. (2017).
Video Game Sales with Ratings. Kaggle.
https://www.kaggle.com/datasets/rush4ratio/video-game-sales-with-ratings/data

Software Packages

- Kuhn, M. (2023). caret: Classification and Regression Training.
- Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System.
- Wright, M. N., & Ziegler, A. (2017). ranger: Random Forests for High Dimensional Data.
- Friedman, J., Hastie, T., & Tibshirani, R. glmnet: Regularized Linear Models.
- Wickham, H. (2023). tidyverse: Data Science Suite.

